## 선형 회귀

1. K-NN

주변 K개의 데이터 셋의 비교를 통해서 결정

장점 :

모델이 직관적이고 구축하기 쉽다.

단점 :

데이터가 크고 피쳐의 개수가 많다면 느리다.

2. Linear Regression

`L(X) = X * W + b` 라는 선형 회귀 방법이다.

장점 : 

특성의 개수가 많고 데이터 사이즈가 크다면 비교적 잘 예측한다.

단점 : 

데이터 사이즈가 적을 경우 과적합이 발생할 확률이 높다.

3. Ridge

`Ridge(X) = L(X)`로 예측 함수 자체는 선형 회귀 모델과 동일 하다. 하지만 선형 회귀에서 각 weight가 결과에 미치는 영향이 너무 큰 경후 과대 적합이 발생할 수 있기 때문에 이를 줄이고자 optimizer 부분에서 제한을 건 모델을 의미한다. 여기서 제한은 `L2` 제한을 사용한다.

장점 :

단순 선형회귀 모델 보다 적은 데이터 셋에서 과대적합을 피할 수 있다.

단점 :

데이터 셋이 많아질 수록 `alpha`의 크기가 의미가 없어진다.

4. Lasso

`Ridge`가 `L2`를 제한 조건으로 사용한 반면 `Lasso`는 `L1`을 제한 조건으로 사용한다. 따라서 여러 특성 중에 진짜 필요한 특성을 제외하고 모두 0으로 수렴하는 결과를 보여준다.

장점 :

특성이 너무 많을때, 어떤 특성이 결과와 연관성이 강한지 판단할 수 있다.

단점 :

Ridge를 주로 사용한다.



## 선형 분류

1. Logistic Regression

선형 분류 방식의 원조? 격이라 할 수 있는 분류 모델. 파라미터`C`를 사용하여 모델의 복잡도를 정할 수 있다. 또한 `penalty` 파라미터를 통해서 `L1` 또는 `L2`를 정할 수 있다.

장점 : 

선형 분류 모델이지만 다중 분류 모델로도 사용가능 하다.

단점 : 

나도 몰라...

2. Linear Support Vector Machine

선형 분류 모델의 두번째 대표 주자. 사실 SVM만 들어가면 내가 안배운 내용같아서 좀 그랬는데 이번 기회에 배울 수 있었다. 

잠점 :

나도 몰라...

단점 :

나도 몰라...



**선형 모델 정리**

선형 모델은 모델의 복잡도를 결정하는 것은 회귀에서는 `alpha` 분류에서는 `C`값이다. 또한 `L1`과 `L2`를 구분해서 사용할 수 있다. `L1`은 피쳐 중에 사용될 특성의 개수가 적을 때 사용하는것이 좋다. 사용되는 특성의 개수가 적기 때문에 학습의 결과를 이해하기 쉽다.

선형 모델의 장점은 크게 3가지이다. 

1. 학습이 빠르다. 더 빠른 학습을 위해서는 `solver="sgd"` 옵션을 주거나 `SGDClassifier`를 사용하는 것이 좋다.
2. 이해가 쉽다. 하지만 변수간의 연관성이 높은 모델의 경우 이해하는것이 어려울 수 있다.
3. 특성의 개수가 샘플의 개수보다 많을 때 효과가 좋다. 



## Naive Bayes 분류기

나이브 베이즈 분류기는 선형 모델과 거의 유사하다. 훈련, 예측 속도가 빠르고, 모델 결과를 이해하기 쉬우며, 희소한 데이터 셋에서 잘 작동한다. 나이브 베이즈 모델은 선형 모델보다 학습하는 시간은 적지만 성능이 조금 떨어지는 것으로 나타난다. 따라서 빠르게 학습하고 싶을 때 사용하면 된다.

나이브 베이즈 모델의 종류는 크게 3가지이다.

1. GaussianNB: 연속적인 데이터를 사용하며 각 클래스별 특성의 평균, 표준편차를 저장해서 사용한다. 주로 고차원 데이터셋을 학습시키는 것에 적합하다. 
2. MulinomialNB: 어떤 것을 카운트한 데이터셋에 사용도니다. 각 클래스별 특성의 평균을 저장해서 사용한다. 주로 텍스트 테이터 셋에  사용되고 BernoulliNB보다 큰 문서에 사용된다.
3. BernoulliNB: 바이너리 데이터 셋에 사용된다. 각 클래스별 특성에서 0이 아닌것의 개수를 저장해서 사용한다. 주로 작은 텍스트 테이터 셋에 사용하는 것이 좋다.



## 트리

트리의 학습은 가장 빠른 방법의 "예/아니오" 질문들로 정답에 도달하는 질문 세트를 학습 하는 것을 의미한다. 여기서 질문 세트란 주로 테스트라고 불린다. 이 때 각 특징에 대해서 테스트를 만들 때 연속적인 값에 대해서는 `>,=,<`과 같은 연산을 통해서 질문을 만들 수 있다. 

트리의 복잡도를 조절하기 위해서는 노드의 개수를 조절 해야 한다. 노드의 개수를 조절하는 방법은 `pre-truning`, `post-truning`과 같은 방법이 있다. `pre-truning`방법에는 트리의 높이를 제한한다거나 데이터의 최소 개수를 조절하는 방법등이 있다.

트리의 학습 결과를 분석하는 방법 중의 하나는 각 특성의 중요도를 확인하는 것 입니다. 특성 중요도를 통해서 어떤 요인이 영향을 크게 미치는지 확인할 수 있습니다. 다만, 각 특성의 중요도와 특정 클래스와 연관지어서 생각하는 것에는 무리가 있습니다.

트리의 특징중 하나가 extrapolation, 즉 훈련 데이터 범위 밖의 포인트에 대해서는 잘 예측 하지 못합니다. 이는 당연한 소리입니다. 예측 할 때 미리 만들어 놓은 질문 범주 안에 없는 데이터에 대해서는 당연히 예측을 할 수 없습니다.

장점 :

모델 결과를 이해하기 쉽다.

단점 :

과적합 되기 쉽다.



## 앙상블

1. RandomForest

랜덤포레스트는 하나의 트리가 아닌 여러개의 트리를 사용해서 단일 트리를 사용할 수 있는 여러 문제점을 해결하고자 만든 기법이다. 단일 트리를 사용할 경우 이해하기 쉽고 빠르다는 장점이 있지만 과적합되기 쉽다는 단점이 있다. 랜덤 포레스트는 하나의 트리가 아닌 여러개의 트리를 사용해서 각각 트리의 예측값을 사용해 하나의 예측값으로 사용하는 모델이다. 여러개의 트리를 만들어서 과적합을 피할 수 있다. 랜덤포레스트의 복잡도를 정하는 것은 여러 요인이 있는데 그 중 중요한 3가지는 `N_ESTIMATOR`, `N_SAMPLES`, `N_FEATURES`와 같은 값들이 있다. 그 중에서도 `N_FEATURES`는 각 트리에서 사용할 특성의 개수를 조절하는 값인데 이 값을 잘 조절해야 과적합을 피하고 각각의 데이터에 맞춘 랜덤포레스트로 학습 할 수 있다.

장점 : 

성능이 좋다.

단점 : 

차원이 높고 희소한 데이터셋에서는 잘 작동하지 않는다.

선형 모델보다 시간이 많이들고 많은 자원을 소비한다.



2. 그레이디언트 부스팅 회귀 트리

여러개의 트리를 함께 쓰는 또다른 기법인 앙상블 기법은 작은 트리를 여러개 사용하는 기법입니다. 

장점 : 

랜덤 포레스트 모델 보다 빠르다.

단점 : 

학습 시간이 길고 차원이 높고 희소한 데이터셋에서는 잘 작동하지 않는다.



3. 배깅

Bootstraping Aggregating의 줄임말이다. 즉 랜럼 샘플링을 바탕으로 훈련세트를 만들고 이를 각기 여러 모델에 학습 시키는 것이다.



4. 에이다부스트

에이다 부스트는 그레이디언트 부스팅과 같은 약한 학습기를 사용하는 모델이다. 



5. 엑스트라 트리

엑스트라 트리는 랜덤 포레스트와 비슷하다. 후보 특성을 랜덤 분할 한 다음 최적의 분할을 찾는 것이다.



## SVM

서포트 벡터 머신은 Decision Boundary에 영향을 미치는 점들을 Support Vector라 하고 이 벡터들과 새로운 점들과의 거리를 측정해서 분류하는 것을 의미한다. 사실 나도 자세히는 모르겠다. 그냥 그렇다고 한다. 이 모델은 앞에서 linear SVM으로 사용하기도 했는데 여기서 말하는 것은 Kernelized SVM을 의미한다. kernel이란 직접 확장된 특성을 만드는 것이 아니라 여러가지 확장된 특성을 이용해서 데이터 사이의 거리를 측정하는 방법이 있다. 확장된 특성을 만드는 방법에 따라서 커널의 종류가 달라지는데 대표적으로 다항식 커널과 가우시안 커널등이 있다. 이런 커널을 사용한 모델들의 복잡도를 조절하기 위해서는 `gamma`값과 `C`값을 조절해야 한다. 

장점 : 

비교적 높은 성능

단점 : 

데이터 스케일에 민감하다.

데이터 사이즈가 10,000개가 넘어갈 경우 속도가 저하된다.



## 신경망

신경망은 입력측, 은닉층, 출력층으로 구성된다. 각 층에서는 인풋과 가중치의 연산을 통해서 다음층으로의 아웃풋을 만든다. 신경망의 복잡도는 은닉층의 수, 은닉유닛 개수, alpha값들로 조절 할 수 있다. 이 모델의 중요한 점은 데이터의 스케일을 평균 0, 분산 1로 맞춰야 한다는 점이다. 

장점 :

학습이 잘 됬을 경우 가장 좋은 성능을 낸다.

단점 : 

학습하기 힘들고 데이터 전처리에 민감하다.